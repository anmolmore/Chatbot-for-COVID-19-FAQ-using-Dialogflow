getwd()
rm(list=ls())
dev.off()
try(require(readxl) || install.packages("readxl",dependencies = TRUE))
try(require(MASS) || install.packages("MASS",dependencies = TRUE))
library(readxl)
Segmentation.Data <- read_excel("OfficeStar Data (Segmentation).xls", sheet = "Segmentation Data", skip = 3, col_names = TRUE)
OfficeStarCluster <- kmeans(Segmentation.Data[,2:7],3)
OfficeStarCluster
OfficeStarCluster$cluster
Discrimination.Data <- read_excel("OfficeStar Data (Segmentation).xls", sheet = "Discrimination Data", skip = 3, col_names = TRUE)
Discrimination.Data$Professional <- factor(Discrimination.Data$Professional)
View(Discrimination.Data)
Discrimination.Data$officegroup <- OfficeStarCluster$cluster
View(Discrimination.Data)
View(Discrimination.Data)
officestar.lda <- lda(officegroup ~ ., data=Discrimination.Data[,2:5])
officestar.lda
officestar.lda.values <- predict(officestar.lda)
#officestar.lda.values
Discrimination.Data$predictedgroup <- officestar.lda.values$class
ConfusionMatrix <- table(Discrimination.Data$officegroup,Discrimination.Data$predictedgroup)
ConfusionMatrix
ConfusionMatrix.in.Percentage <- round(prop.table(ConfusionMatrix,2) * 100,2)
ConfusionMatrix.in.Percentage
Correct.Predictions <- ConfusionMatrix[1,1] + ConfusionMatrix[2,2] + ConfusionMatrix[3,3]
hitrate <- round(Correct.Predictions / length(Discrimination.Data$officegroup) * 100, 2)
knitr::opts_chunk$set(echo = TRUE)
data <- read_excel('Beer Partworth Data.xls')
setwd("~/OneDrive - Indian School of Business/Github/ISB-Term3/Marketing Analytics")
data <- read_excel('Beer Partworth Data.xls')
data <- read_excel('Beer Partworth Data.xls')
data <- read_excel('Beer Partworth Data.xls', sheet = 'Conjoint Data')
#Ref : https://www.r-bloggers.com/finding-optimal-number-of-clusters/
data <- read_excel('Beer Partworth Data.xls', sheet = 'Conjoint Data', skip=13, n_max = 315)
View(data)
#Ref : https://www.r-bloggers.com/finding-optimal-number-of-clusters/
data <- read_excel('Beer Partworth Data.xls', sheet = 'Conjoint Data', skip=13, n_max = 317)
#Ref : https://www.r-bloggers.com/finding-optimal-number-of-clusters/
data <- read_excel('Beer Partworth Data.xls', sheet = 'Conjoint Data', skip=13, n_max = 316)
#Ref : https://www.r-bloggers.com/finding-optimal-number-of-clusters/
data <- read_excel('Beer Partworth Data.xls', sheet = 'Conjoint Data', skip=13, n_max = 315)
#Ref : https://www.r-bloggers.com/finding-optimal-number-of-clusters/
data <- read_excel('Beer Partworth Data.xls', sheet = 'Conjoint Data', skip=13, n_max = 317)
View(data)
#Ref : https://www.r-bloggers.com/finding-optimal-number-of-clusters/
#read conjoint data
data <- read_excel('Beer Partworth Data.xls', sheet = 'Conjoint Data', skip=13, n_max = 317)
data_formatted = as.matrix(scale(data))
View(data_formatted)
k.max <- 15
data <- data_formatted
wss <- sapply(1:k.max,
function(k){kmeans(data, k, nstart=50,iter.max = 15 )$tot.withinss})
wss
plot(1:k.max, wss,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
# Elbow method
fviz_nbclust(data, kmeans, method = "wss") +
geom_vline(xintercept = 4, linetype = 2)+
labs(subtitle = "Elbow method")
install.packages("factoextra")
install.packages("NbClust")
library(factoextra)
library(NbClust)
# Elbow method
fviz_nbclust(data, kmeans, method = "wss") +
geom_vline(xintercept = 4, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(df, kmeans, method = "silhouette")+
labs(subtitle = "Silhouette method")
library(factoextra)
library(NbClust)
# Elbow method
fviz_nbclust(data, kmeans, method = "wss") +
geom_vline(xintercept = 4, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(data, kmeans, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(123)
fviz_nbclust(data, kmeans, nstart = 25,  method = "gap_stat", nboot = 50)+
labs(subtitle = "Gap statistic method")
#Ref : https://www.r-bloggers.com/finding-optimal-number-of-clusters/
#read conjoint data
data <- read_excel('Beer Partworth Data.xls', sheet = 'Conjoint Data', skip=13, n_max = 317)
data_scaled = as.matrix(scale(data))
library(factoextra)
library(NbClust)
# Elbow method
fviz_nbclust(data_scaled, kmeans, method = "wss") +
geom_vline(xintercept = 4, linetype = 2)+
labs(subtitle = "Elbow method")
# Silhouette method
fviz_nbclust(data_scaled, kmeans, method = "silhouette")+
labs(subtitle = "Silhouette method")
# Gap statistic
# nboot = 50 to keep the function speedy.
# recommended value: nboot= 500 for your analysis.
# Use verbose = FALSE to hide computing progression.
set.seed(123)
fviz_nbclust(data_scaled, kmeans, nstart = 25,  method = "gap_stat", nboot = 50)+
labs(subtitle = "Gap statistic method")
wineBest <- FitKMeans(data, max.clusters=20, nstart=25, + seed=278613) > wineBest
install.packages("useful")
library(useful)
wineBest <- FitKMeans(data, max.clusters=20, nstart=25, + seed=278613) > wineBest
library(useful)
wineBest <- FitKMeans(data, max.clusters=20, nstart=25)
library(useful)
wineBest <- FitKMeans(data, max.clusters=20, nstart=25)
PlotHartigan(wineBest)
k.max <- 15
wss <- sapply(1:k.max,
function(k){kmeans(data_scaled, k, nstart=50,iter.max = 15 )$tot.withinss})
wss
plot(1:k.max, wss,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
d_clust <- Mclust(as.matrix(data_scaled), G=1:15,
modelNames = mclust.options("emModelNames"))
install.packages("mclust")
k.max <- 15
wss <- sapply(1:k.max,
function(k){kmeans(data_scaled, k, nstart=50,iter.max = 15 )$tot.withinss})
wss
plot(1:k.max, wss,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
d_clust <- Mclust(as.matrix(data_scaled), G=1:15,
modelNames = mclust.options("emModelNames"))
k.max <- 15
wss <- sapply(1:k.max,
function(k){kmeans(data_scaled, k, nstart=50,iter.max = 15 )$tot.withinss})
wss
plot(1:k.max, wss,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
library(mclust)
d_clust <- Mclust(as.matrix(data_scaled), G=1:15,
modelNames = mclust.options("emModelNames"))
d_clust$BIC
plot(d_clust)
k.max <- 15
wss <- sapply(1:k.max,
function(k){kmeans(data_scaled, k, nstart=50,iter.max = 15 )$tot.withinss})
wss
plot(1:k.max, wss,
type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares")
library(mclust)
d_clust <- Mclust(as.matrix(data_scaled), G=1:15,
modelNames = mclust.options("emModelNames"))
d_clust$BIC
plot(d_clust)
install.packages("NbClust",dependencies = TRUE)
library(NbClust)
nb <- NbClust(data_scaled, diss=NULL, distance = "euclidean",
min.nc=2, max.nc=5, method = "kmeans",
index = "all", alphaBeale = 0.1)
hist(nb$Best.nc[1,], breaks = max(na.omit(nb$Best.nc[1,])))
install.packages("NbClust", dependencies = TRUE)
