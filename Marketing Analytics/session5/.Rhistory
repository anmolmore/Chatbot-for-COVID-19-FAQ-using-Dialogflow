library(factoextra)
library(corrplot)
#describe data
library(Hmisc)
#plotting trees
library(rpart)
library(rpart.plot)
library(randomForest)
knitr::opts_chunk$set(echo = TRUE)
conjoint_data <- readxl::read_excel('Beer Partworth Data.xls', sheet = 'Conjoint Data', skip=13, n_max = 317)
#skip the first column which contains respondent IDs, not required for clustering the data
conjoint_data <- conjoint_data[,-c(1)]
#Considering relevance and trying out various algorithms, we limit the no of clusters to 6
max_k <- 6
#calculate within cluster square sum differennces for each k
wc_ss <- sapply(1:max_k,
function(k){kmeans(conjoint_data, centers = k, iter.max = 15, nstart=25 )$tot.withinss})
plot(1:max_k, wc_ss,
type='l',
xlab="Number of clusters K",
ylab="Total within-cluster sum of squares, i.e. sum(withinss)")
#calculate between cluster square sum differennces for each k
bc_ss <- sapply(1:max_k,
function(k){kmeans(conjoint_data, centers = k, iter.max = 15, nstart=25 )$betweenss})
plot(1:max_k, bc_ss,
type='l',
xlab="Number of clusters K",
ylab="The between-cluster sum of squares, i.e. totss-tot.withinss.")
# Elbow Method
fviz_nbclust(conjoint_data, kmeans, method = "wss")
labs(subtitle = "Elbow Method")
# Silhouette Method
fviz_nbclust(conjoint_data, kmeans, method = "silhouette") +
geom_vline(xintercept = 2, linetype = 2) +
labs(subtitle = "Silhouette Method")
# Gap statistic method doesn't gives a clear k
fviz_nbclust(conjoint_data, kmeans, method = "gap_stat")
labs(subtitle = "Gap Statistic Method")
demographics_data <- readxl::read_excel('Beer Partworth Data.xls', sheet = 'Demographics', skip=3, n_max = 317)
#use two clusters
demographics_data$Cluster <- kmeans(conjoint_data, 2)$cluster
#skip first column containing ID of respondents and clusters from conjoint data
data <- demographics_data[,-c(1)]
#correlation plot surprisingly doesn't shows much correlation between Age, Income, Education or even with cluster (the target variable)
corrplot(cor(data), method = "number")
write.csv(data, file = "clustering data.csv", row.names=FALSE)
names(data) <- c('Weekly_Consumption', 'Age_Group',	'Income_Group',	'Education_Group',	'Sex', 'Cluster')
#data <- read.csv("clustering data.csv", colClasses=c('numeric', 'factor', 'factor', 'factor', 'factor', 'factor'))
plot(data[,c(1,6)], aes('Weekly consumption','Cluster'))
plot(data[,c(2,6)], aes('Age Group (1-7)','Cluster'))
plot(data[,c(3,6)], aes('Income Group (1-7)','Cluster'))
plot(data[,c(4,6)], aes('Education','Cluster'))
plot(data[,c(5,6)], aes('Sex','Cluster'))
describe(data)
data <- read.csv("clustering data.csv", colClasses=c('numeric', 'factor', 'factor', 'factor', 'factor', 'factor'))
names(data) <- c('Weekly_Consumption', 'Age_Group',	'Income_Group',	'Education_Group',	'Sex', 'Cluster')
random_forest_tree = randomForest(Cluster~., data = data)
summary(random_forest_tree)
library(gbm)
gradient_boosted_tree = gbm(Cluster~., data = data, distribution = "gaussian")
View(demographics_data)
summary(gradient_boosted_tree)
# Load rpart and rpart.plot
# Create a decision tree model
tree <- rpart(Cluster~Income_Group, data = data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
# Create a decision tree model
subset_data <- data[data$Income_Group == "2" | data$Income_Group == "3" | data$Income_Group == "4" | data$Income_Group == "5", ]
tree <- rpart(Cluster~Age_Group, data = subset_data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
#subset_data <- data[data$Income_Group == "1" | data$Income_Group == "6" | data$Income_Group == "6", ]
tree <- rpart(Cluster~Weekly_Consumption, data = data, minsplit = 93)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
View(demographics_data)
# Load rpart and rpart.plot
# Create a decision tree model
tree <- rpart(Cluster~Income_Group, data = data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
library(gbm)
gradient_boosted_tree = gbm(Cluster~., data = data, distribution = "gaussian")
summary(gradient_boosted_tree)
# Create a decision tree model
tree <- rpart(Cluster~Income_Group, data = data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
library(factoextra)
library(corrplot)
#to describe data
library(Hmisc)
#for plotting trees
library(rpart)
library(rpart.plot)
library(randomForest)
knitr::opts_chunk$set(echo = TRUE)
conjoint_data <- readxl::read_excel('Beer Partworth Data.xls', sheet = 'Conjoint Data', skip=13, n_max = 317)
#skip the first column which contains respondent IDs, not required for clustering the data
conjoint_data <- conjoint_data[,-c(1)]
#Considering relevance and trying out various algorithms, we limit the no of clusters to 6
max_k <- 6
#calculate within cluster square sum differennces for each k
wc_ss <- sapply(1:max_k,
function(k){kmeans(conjoint_data, centers = k, iter.max = 15, nstart=25 )$tot.withinss})
plot(1:max_k, wc_ss,
type='l',
xlab="Number of clusters K",
ylab="Total within-cluster sum of squares, i.e. sum(withinss)")
#calculate between cluster square sum differennces for each k
bc_ss <- sapply(1:max_k,
function(k){kmeans(conjoint_data, centers = k, iter.max = 15, nstart=25 )$betweenss})
plot(1:max_k, bc_ss,
type='l',
xlab="Number of clusters K",
ylab="The between-cluster sum of squares, i.e. totss-tot.withinss.")
# Elbow Method
fviz_nbclust(conjoint_data, kmeans, method = "wss")
labs(subtitle = "Elbow Method")
# Silhouette Method
fviz_nbclust(conjoint_data, kmeans, method = "silhouette") +
geom_vline(xintercept = 2, linetype = 2) +
labs(subtitle = "Silhouette Method")
# Gap statistic method doesn't gives a clear k
fviz_nbclust(conjoint_data, kmeans, method = "gap_stat")
labs(subtitle = "Gap Statistic Method")
demographics_data <- readxl::read_excel('Beer Partworth Data.xls', sheet = 'Demographics', skip=3, n_max = 317)
#use two clusters
demographics_data$Cluster <- kmeans(conjoint_data, 2)$cluster
#skip first column containing ID of respondents and clusters from conjoint data
data <- demographics_data[,-c(1)]
#correlation plot surprisingly doesn't shows much correlation between Age, Income, Education or even with cluster (the target variable)
corrplot(cor(data), method = "number")
write.csv(data, file = "clustering data.csv", row.names=FALSE)
names(data) <- c('Weekly_Consumption', 'Age_Group',	'Income_Group',	'Education_Group',	'Sex', 'Cluster')
#data <- read.csv("clustering data.csv", colClasses=c('numeric', 'factor', 'factor', 'factor', 'factor', 'factor'))
plot(data[,c(1,6)], aes('Weekly consumption','Cluster'))
plot(data[,c(2,6)], aes('Age Group (1-7)','Cluster'))
plot(data[,c(3,6)], aes('Income Group (1-7)','Cluster'))
plot(data[,c(4,6)], aes('Education','Cluster'))
plot(data[,c(5,6)], aes('Sex','Cluster'))
describe(data)
data <- read.csv("clustering data.csv", colClasses=c('numeric', 'factor', 'factor', 'factor', 'factor', 'factor'))
names(data) <- c('Weekly_Consumption', 'Age_Group',	'Income_Group',	'Education_Group',	'Sex', 'Cluster')
random_forest_tree = randomForest(Cluster~., data = data)
summary(random_forest_tree)
library(gbm)
gradient_boosted_tree = gbm(Cluster~., data = data, distribution = "gaussian")
summary(gradient_boosted_tree)
# Create a decision tree model
tree <- rpart(Cluster~Income_Group, data = data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
# Create a decision tree model
subset_data <- data[data$Income_Group == "2" | data$Income_Group == "3" | data$Income_Group == "4" | data$Income_Group == "5", ]
tree <- rpart(Cluster~Age_Group, data = subset_data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
#subset_data <- data[data$Income_Group == "1" | data$Income_Group == "6" | data$Income_Group == "6", ]
tree <- rpart(Cluster~Weekly_Consumption, data = data, minsplit = 93)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
subset_data <- data[data$Age_Group == "1" | data$Income_Group == "2" | data$Income_Group == "3" | data$Income_Group == "4"| data$Income_Group == "5", ]
tree <- rpart(Cluster~Income_Group, data = subset_data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
subset_data <- data[data$Age_Group == "1" | data$Age_Group == "2" | data$Age_Group == "3" | data$Age_Group == "4"| data$Age_Group == "5", ]
tree <- rpart(Cluster~Income_Group, data = subset_data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
subset_data <- data[data$Age_Group == "1" | data$Age_Group == "2" | data$Age_Group == "3" | data$Age_Group == "4"| data$Age_Group == "5", ]
tree <- rpart(Cluster~Weekly_Consumption, data = subset_data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
View(subset_data)
subset_data <- data[data$Income_Group == "1" | data$Income_Group == "6", ]
#subset_data <- data[data$Age_Group == "6" | data$Age_Group == "7", ]
tree <- rpart(Cluster~Weekly_Consumption, data = subset_data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
View(subset_data)
subset_data <- data[data$Age_Group == "5" | data$Age_Group == "6", ]
#subset_data <- data[data$Age_Group == "6" | data$Age_Group == "7", ]
tree <- rpart(Cluster~Weekly_Consumption, data = subset_data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
View(subset_data)
subset_data <- data[data$Age_Group == "5" | data$Age_Group == "6", ]
#subset_data <- data[data$Age_Group == "6" | data$Age_Group == "7", ]
tree <- rpart(Cluster~Weekly_Consumption, data = subset_data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
# Create a decision tree model
subset_data <- data[data$Income_Group == "2" | data$Income_Group == "3" | data$Income_Group == "4" | data$Income_Group == "5", ]
tree <- rpart(Cluster~Weekly_Consumption, data = subset_data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
# Create a decision tree model
tree <- rpart(Cluster~Age_Group, data = data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
library(factoextra)
library(corrplot)
#to describe data
library(Hmisc)
#for plotting trees
library(rpart)
library(rpart.plot)
library(randomForest)
knitr::opts_chunk$set(echo = TRUE)
conjoint_data <- readxl::read_excel('Beer Partworth Data.xls', sheet = 'Conjoint Data', skip=13, n_max = 317)
#skip the first column which contains respondent IDs, not required for clustering the data
conjoint_data <- conjoint_data[,-c(1)]
#Considering relevance and trying out various algorithms, we limit the no of clusters to 6
max_k <- 6
#calculate within cluster square sum differennces for each k
wc_ss <- sapply(1:max_k,
function(k){kmeans(conjoint_data, centers = k, iter.max = 15, nstart=25 )$tot.withinss})
plot(1:max_k, wc_ss,
type='l',
xlab="Number of clusters K",
ylab="Total within-cluster sum of squares, i.e. sum(withinss)")
#calculate between cluster square sum differennces for each k
bc_ss <- sapply(1:max_k,
function(k){kmeans(conjoint_data, centers = k, iter.max = 15, nstart=25 )$betweenss})
plot(1:max_k, bc_ss,
type='l',
xlab="Number of clusters K",
ylab="The between-cluster sum of squares, i.e. totss-tot.withinss.")
# Elbow Method
fviz_nbclust(conjoint_data, kmeans, method = "wss")
labs(subtitle = "Elbow Method")
# Silhouette Method
fviz_nbclust(conjoint_data, kmeans, method = "silhouette") +
geom_vline(xintercept = 2, linetype = 2) +
labs(subtitle = "Silhouette Method")
# Gap statistic method doesn't gives a clear k
fviz_nbclust(conjoint_data, kmeans, method = "gap_stat")
labs(subtitle = "Gap Statistic Method")
demographics_data <- readxl::read_excel('Beer Partworth Data.xls', sheet = 'Demographics', skip=3, n_max = 317)
#use two clusters
demographics_data$Cluster <- kmeans(conjoint_data, 2)$cluster
#skip first column containing ID of respondents and clusters from conjoint data
data <- demographics_data[,-c(1)]
#correlation plot surprisingly doesn't shows much correlation between Age, Income, Education or even with cluster (the target variable)
corrplot(cor(data), method = "number")
write.csv(data, file = "clustering data.csv", row.names=FALSE)
names(data) <- c('Weekly_Consumption', 'Age_Group',	'Income_Group',	'Education_Group',	'Sex', 'Cluster')
#data <- read.csv("clustering data.csv", colClasses=c('numeric', 'factor', 'factor', 'factor', 'factor', 'factor'))
plot(data[,c(1,6)], aes('Weekly consumption','Cluster'))
plot(data[,c(2,6)], aes('Age Group (1-7)','Cluster'))
plot(data[,c(3,6)], aes('Income Group (1-7)','Cluster'))
plot(data[,c(4,6)], aes('Education','Cluster'))
plot(data[,c(5,6)], aes('Sex','Cluster'))
describe(data)
data <- read.csv("clustering data.csv", colClasses=c('numeric', 'factor', 'factor', 'factor', 'factor', 'factor'))
names(data) <- c('Weekly_Consumption', 'Age_Group',	'Income_Group',	'Education_Group',	'Sex', 'Cluster')
random_forest_tree = randomForest(Cluster~., data = data)
summary(random_forest_tree)
library(gbm)
gradient_boosted_tree = gbm(Cluster~., data = data, distribution = "gaussian")
summary(gradient_boosted_tree)
# Create a decision tree model
tree <- rpart(Cluster~Age_Group, data = data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
# Create a decision tree model
subset_data <- data[data$Income_Group == "2" | data$Income_Group == "3" | data$Income_Group == "4" | data$Income_Group == "5", ]
tree <- rpart(Cluster~Weekly_Consumption, data = subset_data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
subset_data <- data[data$Age_Group == "1" | data$Age_Group == "2" | data$Age_Group == "3" | data$Age_Group == "4"| data$Age_Group == "5", ]
tree <- rpart(Cluster~Weekly_Consumption, data = subset_data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
subset_data <- data[data$Age_Group == "5" | data$Age_Group == "6", ]
#subset_data <- data[data$Age_Group == "6" | data$Age_Group == "7", ]
tree <- rpart(Cluster~Weekly_Consumption, data = subset_data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
# Create a decision tree model
tree <- rpart(Cluster~Age_Group, data = data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
library(factoextra)
library(corrplot)
#to describe data
library(Hmisc)
#for plotting trees
library(rpart)
library(rpart.plot)
library(randomForest)
knitr::opts_chunk$set(echo = TRUE)
conjoint_data <- readxl::read_excel('Beer Partworth Data.xls', sheet = 'Conjoint Data', skip=13, n_max = 317)
#skip the first column which contains respondent IDs, not required for clustering the data
conjoint_data <- conjoint_data[,-c(1)]
#Considering relevance and trying out various algorithms, we limit the no of clusters to 6
max_k <- 6
#calculate within cluster square sum differennces for each k
wc_ss <- sapply(1:max_k,
function(k){kmeans(conjoint_data, centers = k, iter.max = 15, nstart=25 )$tot.withinss})
plot(1:max_k, wc_ss,
type='l',
xlab="Number of clusters K",
ylab="Total within-cluster sum of squares, i.e. sum(withinss)")
#calculate between cluster square sum differennces for each k
bc_ss <- sapply(1:max_k,
function(k){kmeans(conjoint_data, centers = k, iter.max = 15, nstart=25 )$betweenss})
plot(1:max_k, bc_ss,
type='l',
xlab="Number of clusters K",
ylab="The between-cluster sum of squares, i.e. totss-tot.withinss.")
# Elbow Method
fviz_nbclust(conjoint_data, kmeans, method = "wss")
labs(subtitle = "Elbow Method")
# Silhouette Method
fviz_nbclust(conjoint_data, kmeans, method = "silhouette") +
geom_vline(xintercept = 2, linetype = 2) +
labs(subtitle = "Silhouette Method")
# Gap statistic method doesn't gives a clear k
fviz_nbclust(conjoint_data, kmeans, method = "gap_stat")
labs(subtitle = "Gap Statistic Method")
demographics_data <- readxl::read_excel('Beer Partworth Data.xls', sheet = 'Demographics', skip=3, n_max = 317)
#use two clusters
demographics_data$Cluster <- kmeans(conjoint_data, 2)$cluster
#skip first column containing ID of respondents and clusters from conjoint data
data <- demographics_data[,-c(1)]
#correlation plot surprisingly doesn't shows much correlation between Age, Income, Education or even with cluster (the target variable)
corrplot(cor(data), method = "number")
write.csv(data, file = "clustering data.csv", row.names=FALSE)
names(data) <- c('Weekly_Consumption', 'Age_Group',	'Income_Group',	'Education_Group',	'Sex', 'Cluster')
#data <- read.csv("clustering data.csv", colClasses=c('numeric', 'factor', 'factor', 'factor', 'factor', 'factor'))
plot(data[,c(1,6)], aes('Weekly consumption','Cluster'))
plot(data[,c(2,6)], aes('Age Group (1-7)','Cluster'))
plot(data[,c(3,6)], aes('Income Group (1-7)','Cluster'))
plot(data[,c(4,6)], aes('Education','Cluster'))
plot(data[,c(5,6)], aes('Sex','Cluster'))
describe(data)
data <- read.csv("clustering data.csv", colClasses=c('numeric', 'factor', 'factor', 'factor', 'factor', 'factor'))
names(data) <- c('Weekly_Consumption', 'Age_Group',	'Income_Group',	'Education_Group',	'Sex', 'Cluster')
random_forest_tree = randomForest(Cluster~., data = data)
summary(random_forest_tree)
library(gbm)
gradient_boosted_tree = gbm(Cluster~., data = data, distribution = "gaussian")
summary(gradient_boosted_tree)
# Create a decision tree model
tree <- rpart(Cluster~Age_Group, data = data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
View(data)
# Create a decision tree model
tree <- rpart(Cluster~., data = data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
subset_data <- data[data$Age_Group == "1" | data$Age_Group == "2" | data$Age_Group == "3" | data$Age_Group == "4"| data$Age_Group == "5", ]
tree <- rpart(Cluster~Weekly_Consumption, data = subset_data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
subset_data <- data[data$Age_Group == "1" | data$Age_Group == "2" | data$Age_Group == "3" | data$Age_Group == "4"| data$Age_Group == "5", ]
tree <- rpart(Cluster~Weekly_Consumption, data = subset_data)
# Visualize the tree in younger age group
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
subset_data <- data[data$Age_Group == "5" | data$Age_Group == "6", ]
tree <- rpart(Cluster~Weekly_Consumption, data = subset_data)
# Visualize tree for weekly consumption in older age group
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
# Create a decision tree to differentiate between different age groups
tree <- rpart(Cluster~Income_Group, data = subset_data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
# Create a decision tree to differentiate between different age groups
tree <- rpart(Cluster~Income_Group, data = data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
subset_data <- data[data$Age_Group == "1" | data$Age_Group == "2" | data$Age_Group == "3" | data$Age_Group == "4"| data$Age_Group == "5", ]
subset_data <- data[data$Cluster == "2", ]
tree <- rpart(Cluster~Weekly_Consumption, data = subset_data)
# Visualize the tree in younger age group
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
subset_data <- data[data$Age_Group == "1" | data$Age_Group == "2" | data$Age_Group == "3" | data$Age_Group == "4"| data$Age_Group == "5", ]
tree <- rpart(Cluster~Weekly_Consumption, data = subset_data)
# Visualize the tree in younger age group
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
plot(data[,c(1,6)], type='b', aes('Weekly consumption','Cluster'))
barplot(data[,c(1,6)], aes('Weekly consumption','Cluster'))
plot(data[,c(2,6)], type='h', aes('Age Group (1-7)','Cluster'))
barplot(data[,c(1,6)])
barplot(data[ ,6], names.arg = data[ ,1])
barplot(data[ ,1], names.arg = data[ ,6])
barplot(data[ ,2], names.arg = data[ ,6])
barplot(data[ ,2], names.arg = data[ ,6])
