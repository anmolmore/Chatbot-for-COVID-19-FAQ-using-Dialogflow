---
title: "MA Group Assignment"
output:
  html_document:
    df_print: paged
  word_document: default
---

Anmol More – 11915043

Sriganesh Balamurugan – 11915001

Raghu Punnamraju – 11915010

Ref : 
- https://www.r-bloggers.com/finding-optimal-number-of-clusters/

- https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/

- https://medium.com/codesmart/r-series-k-means-clustering-silhouette-794774b46586

```{r import required libraries, include=FALSE}
library(factoextra)
library(corrplot)
#to describe data
library(Hmisc)

#for plotting trees
library(rpart)
library(rpart.plot)
library(randomForest)
knitr::opts_chunk$set(echo = TRUE)
```

### Segmentation

## 1. Segment respondents based on the Partworth data (use any unsupervised learning technique).

```{r read conjoint data}
conjoint_data <- readxl::read_excel('Beer Partworth Data.xls', sheet = 'Conjoint Data', skip=13, n_max = 317)

#skip the first column which contains respondent IDs, not required for clustering the data
conjoint_data <- conjoint_data[,-c(1)]
```

# First we apply various clustering algorithms to come up with best K.

In order to find best K, we looked at within cluster and between cluster Sum of square differences for k. 

```{r apply clustering techniques, echo=FALSE}

#Considering relevance and trying out various algorithms, we limit the no of clusters to 6
max_k <- 6

#calculate within cluster square sum differennces for each k
wc_ss <- sapply(1:max_k, 
              function(k){kmeans(conjoint_data, centers = k, iter.max = 15, nstart=25 )$tot.withinss})
plot(1:max_k, wc_ss,
     type='l',
     xlab="Number of clusters K",
     ylab="Total within-cluster sum of squares, i.e. sum(withinss)")

#calculate between cluster square sum differennces for each k
bc_ss <- sapply(1:max_k, 
              function(k){kmeans(conjoint_data, centers = k, iter.max = 15, nstart=25 )$betweenss})
plot(1:max_k, bc_ss,
     type='l',
     xlab="Number of clusters K",
     ylab="The between-cluster sum of squares, i.e. totss-tot.withinss.")
```

Looking at within cluster and between cluster differences, we can say that data can be divided in either 2 or 3 clusters.

We try various k means methods like Elbow, Silhouette and find 2 to be most optimal clustering

```{r}
# Elbow Method
fviz_nbclust(conjoint_data, kmeans, method = "wss")
  labs(subtitle = "Elbow Method")

# Silhouette Method
fviz_nbclust(conjoint_data, kmeans, method = "silhouette") +
  geom_vline(xintercept = 2, linetype = 2) +
  labs(subtitle = "Silhouette Method")

# Gap statistic method doesn't gives a clear k
fviz_nbclust(conjoint_data, kmeans, method = "gap_stat")
  labs(subtitle = "Gap Statistic Method")
```

## 2.	Use the Descriptors in the Demographic data sheet to perform classification (use any supervised learning technique) based on segments obtained in Step 1 and personify /describe each segment.

# Read demographics data and apply clusters obtained from conjoint data on Demographics data

```{r}
demographics_data <- readxl::read_excel('Beer Partworth Data.xls', sheet = 'Demographics', skip=3, n_max = 317)

#use two clusters
demographics_data$Cluster <- kmeans(conjoint_data, 2)$cluster

#skip first column containing ID of respondents and clusters from conjoint data
data <- demographics_data[,-c(1)]
```

# Let's plot various column to understand the distribution

```{r}
#correlation plot surprisingly doesn't shows much correlation between Age, Income, Education or even with cluster (the target variable)
corrplot(cor(data), method = "number")
```

# we see from above correlation plot that, our clusters are not correlated to any variables, to some extent age, income and education are weakly correlated

Let us try to divide two set of people

```{r}
write.csv(data, file = "clustering data.csv", row.names=FALSE)
names(data) <- c('Weekly_Consumption', 'Age_Group',	'Income_Group',	'Education_Group',	'Sex', 'Cluster')
#data <- read.csv("clustering data.csv", colClasses=c('numeric', 'factor', 'factor', 'factor', 'factor', 'factor'))
```

Plot each variable against no. of clusters defined to identify reltionships

```{r plot cluster vs other columns}

pldata = data

#Weekly_Consumption vs Cluster
dat <- data.frame(table(pldata$Weekly_Consumption,pldata$Cluster))
names(dat) <- c("Weekly_Consumption","Cluster","Count")
ggplot(data=dat, aes(x=Weekly_Consumption, y=Count, fill=Cluster)) + geom_bar(stat="identity")

#Age_group vs Cluster
dat <- data.frame(table(pldata$Age_Group,pldata$Cluster))
names(dat) <- c("Age_group","Cluster","Count")
ggplot(data=dat, aes(x=Age_group, y=Count, fill=Cluster)) + geom_bar(stat="identity")

#Income_Group vs Cluster
dat <- data.frame(table(pldata$Income_Group,pldata$Cluster))
names(dat) <- c("Income_Group","Cluster","Count")
ggplot(data=dat, aes(x=Income_Group, y=Count, fill=Cluster)) + geom_bar(stat="identity")

#Education_Group vs Cluster
dat <- data.frame(table(pldata$Education_Group,pldata$Cluster))
names(dat) <- c("Education_Group","Cluster","Count")
ggplot(data=dat, aes(x=Education_Group, y=Count, fill=Cluster)) + geom_bar(stat="identity")

#Sex vs Cluster
dat <- data.frame(table(pldata$Sex,pldata$Cluster))
names(dat) <- c("Sex","Cluster","Count")
ggplot(data=dat, aes(x=Sex, y=Count, fill=Cluster)) + geom_bar(stat="identity")

```


check summary of data to understand distribution of each of the variables

```{r}
describe(data)
```

Run random forest supervised learning technique to classify data

```{r run random forest}
data <- read.csv("clustering data.csv", colClasses=c('numeric', 'factor', 'factor', 'factor', 'factor', 'factor'))
names(data) <- c('Weekly_Consumption', 'Age_Group',	'Income_Group',	'Education_Group',	'Sex', 'Cluster')
random_forest_tree = randomForest(Cluster~., data = data)
summary(random_forest_tree)
```

Run gradient boosted trees

```{r run gradient boosted trees}
library(gbm)
gradient_boosted_tree = gbm(Cluster~., data = data, distribution = "gaussian")
summary(gradient_boosted_tree)
```

```{r}
# Create a decision tree model
tree <- rpart(Cluster~., data = data)
# Visualize the decision tree with rpart.plot
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
```

It is evident in below plot that high income group people (75%) fall in cluster 2

```{r}
# Create a decision tree to differentiate between different income groups
tree <- rpart(Cluster~Income_Group, data = data)
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
```

younger population within age group of below 40 have higher weekly consumption than average consumption of 9.4. Around 83% are falling in  falling in segment 1

```{r}
subset_data <- data[data$Age_Group == "1" | data$Age_Group == "2" | data$Age_Group == "3" | data$Age_Group == "4"| data$Age_Group == "5", ]
tree <- rpart(Cluster~Weekly_Consumption, data = subset_data)

# Visualize the tree in younger age group
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
```

Some further analysis on higher age group 40+ age, shows 50%+ fall in cluster 2

```{r}
subset_data <- data[data$Age_Group == "5" | data$Age_Group == "6", ]
tree <- rpart(Cluster~Weekly_Consumption, data = subset_data)

# Visualize tree for weekly consumption in older age group
rpart.plot(tree, box.palette="RdBu", shadow.col="gray")
```

Finally we can divide the whole data in two groups -

1. Young and heavy drinkers - These are people in age group of less than 40, and have average weekly consumption of 10.24 bottles/cans

2. Old and occasional drinkers - These are older people in age group of above 40, and have average weekly consumption of 8.8. Also, we observed that 75% of people in this group earn more than $50k.

So, we can say that older and higher income group people are occasional drinkers and prefer costly brands over younnger lot who are heavy drinkers are prefer cheaper priced brands


